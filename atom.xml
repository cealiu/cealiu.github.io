<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LiuCe&#39;s Blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://cealiu.me/"/>
  <updated>2017-09-04T03:15:21.000Z</updated>
  <id>http://cealiu.me/</id>
  
  <author>
    <name>LiuCe</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>hadoop demo程序</title>
    <link href="http://cealiu.me/2017/09/04/hadoop-demo%E7%A8%8B%E5%BA%8F/"/>
    <id>http://cealiu.me/2017/09/04/hadoop-demo程序/</id>
    <published>2017-09-04T03:06:44.000Z</published>
    <updated>2017-09-04T03:15:21.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Map-Reduce介绍"><a href="#Map-Reduce介绍" class="headerlink" title="Map/Reduce介绍"></a>Map/Reduce介绍</h2><p>hadoop主要利用Map/Reduce框架进行快速数据处理，就是将上传到hadoop集群的文件进行分片保存在HDFS上（64M），之后利用Map框架进行预处理后交由Reduce框架处理输出结果，如下图(简易图)：</p>
<p><img src="\images\MapReduce.jpg" alt=""></p>
<h2 id="工程构建"><a href="#工程构建" class="headerlink" title="工程构建"></a>工程构建</h2><p>利用idea建立maven工程，pom.xml配置如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</div><div class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></div><div class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></div><div class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>HadoopTest<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>HadoopTest<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.8.0<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-jobclient<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></div></pre></td></tr></table></figure>
<p>之后建立WordCount.java编译生成jar文件。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">ackage org.myorg;</div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"><span class="keyword">import</span> java.util.*;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.*;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.*;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.*;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.util.*;</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Map</span> <span class="keyword">extends</span> <span class="title">MapReduceBase</span> <span class="keyword">implements</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</div><div class="line">    <span class="keyword">private</span> Text word = <span class="keyword">new</span> Text();</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">        String line = value.toString();</div><div class="line">        StringTokenizer tokenizer = <span class="keyword">new</span> StringTokenizer(line);</div><div class="line">        <span class="keyword">while</span> (tokenizer.hasMoreTokens()) &#123;</div><div class="line">            word.set(tokenizer.nextToken());</div><div class="line">            output.collect(word, one);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Reduce</span> <span class="keyword">extends</span> <span class="title">MapReduceBase</span> <span class="keyword">implements</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterator&lt;IntWritable&gt; values, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">            <span class="keyword">int</span> sum = <span class="number">0</span>;</div><div class="line">            <span class="keyword">while</span> (values.hasNext()) &#123;</div><div class="line">                sum += values.next().get();</div><div class="line">            &#125;</div><div class="line">            output.collect(key, <span class="keyword">new</span> IntWritable(sum));</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">        JobConf conf = <span class="keyword">new</span> JobConf(WordCount.class);</div><div class="line">        conf.setJobName(<span class="string">"wordcount"</span>);</div><div class="line">        conf.setOutputKeyClass(Text.class);</div><div class="line">        conf.setOutputValueClass(IntWritable.class);</div><div class="line">        conf.setMapperClass(Map.class);</div><div class="line">        conf.setCombinerClass(Reduce.class);</div><div class="line">        conf.setReducerClass(Reduce.class);</div><div class="line"></div><div class="line">        conf.setInputFormat(TextInputFormat.class);</div><div class="line">        conf.setOutputFormat(TextOutputFormat.class);</div><div class="line"></div><div class="line">        FileInputFormat.setInputPaths(conf, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</div><div class="line">        FileOutputFormat.setOutputPath(conf, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</div><div class="line"></div><div class="line">        JobClient.runJob(conf);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这段代码主要实现了map／reduce处理过程，上节利用命令 -put上传的文件被分配到各个datanode节点。</p>
<p>public void map()按文件行分解为单词输出key/value值</p>
<p>public void reduce()按map传递过来的值统计单词</p>
<p>之后就是在main函数中配置job</p>
<h2 id="程序运行"><a href="#程序运行" class="headerlink" title="程序运行"></a>程序运行</h2><p>上面生成了HadoopTest-1.0-SNAPSHOT.jar</p>
<p>运行命令，会在/user/liuce/output看到输出结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop jar ./HadoopTest-1.0-SNAPSHOT.jar org.myorg.WordCount /user/liuce/input /user/liuce/output</div></pre></td></tr></table></figure>
<p>eclipse下有插件可以远程连接到hadoop服务器上而不用每次做这些上传删除操作，github上只有2.6版本的高级版本可以自己编译，linux、win下插件都没问题但是macos下一直报错，感觉跟mac系统的目录结构有关一直报找不到job。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Map-Reduce介绍&quot;&gt;&lt;a href=&quot;#Map-Reduce介绍&quot; class=&quot;headerlink&quot; title=&quot;Map/Reduce介绍&quot;&gt;&lt;/a&gt;Map/Reduce介绍&lt;/h2&gt;&lt;p&gt;hadoop主要利用Map/Reduce框架进行快速数据处理
    
    </summary>
    
    
      <category term="bigdata" scheme="http://cealiu.me/tags/bigdata/"/>
    
  </entry>
  
  <entry>
    <title>hadoop单机搭建</title>
    <link href="http://cealiu.me/2017/09/04/hadoop%E5%8D%95%E6%9C%BA%E6%90%AD%E5%BB%BA/"/>
    <id>http://cealiu.me/2017/09/04/hadoop单机搭建/</id>
    <published>2017-09-04T02:52:11.000Z</published>
    <updated>2017-09-04T03:05:50.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Hadoop-安装"><a href="#Hadoop-安装" class="headerlink" title="Hadoop 安装"></a>Hadoop 安装</h2><p>系统macos 10.12.4，linux系统大体与此相似</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">brew insall hadoop</div></pre></td></tr></table></figure>
<p>该命令安装是是最新版（2.8.0）</p>
<p>配置JAVA_HOME(之前已经配置过，java版本1.8)</p>
<h2 id="配置ssh免密码登录"><a href="#配置ssh免密码登录" class="headerlink" title="配置ssh免密码登录"></a><strong>配置ssh免密码登录</strong></h2><p>1、生成公钥，加入authorized_keys</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ssh-keygen -t rsa</div><div class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</div></pre></td></tr></table></figure>
<h2 id="Hadoop-配置单节点使用"><a href="#Hadoop-配置单节点使用" class="headerlink" title="Hadoop 配置单节点使用"></a>Hadoop 配置单节点使用</h2><p>这里是使用单节点，brew install的hadoop目录在</p>
<p>/usr/local/Cellar/hadoop/2.8.0</p>
<p>配置文件目录在</p>
<p>/usr/local/Cellar/hadoop/2.8.0/libexec/etc/hadoop</p>
<h3 id="配置-hdfs-site-xml"><a href="#配置-hdfs-site-xml" class="headerlink" title="配置 hdfs-site.xml"></a>配置 hdfs-site.xml</h3><p>设置副本数为 <strong>1</strong>:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure>
<h3 id="配置-core-site-xml"><a href="#配置-core-site-xml" class="headerlink" title="配置 core-site.xml"></a>配置 core-site.xml</h3><p>设置文件系统访问的端口：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure>
<h3 id="配置-mapred-site-xml"><a href="#配置-mapred-site-xml" class="headerlink" title="配置 mapred-site.xml"></a>配置 mapred-site.xml</h3><p>设置 MapReduce 使用的框架：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure>
<h3 id="配置-yarn-site-xml"><a href="#配置-yarn-site-xml" class="headerlink" title="配置 yarn-site.xml"></a>配置 yarn-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure>
<h2 id="Hadoop运行"><a href="#Hadoop运行" class="headerlink" title="Hadoop运行"></a>Hadoop运行</h2><p>因为没有将hadoop目录环境变量，所以以下命令需要在/usr/local/Cellar/hadoop/2.8.0/libexec/sbin目录下运行。</p>
<h3 id="启动hadoop"><a href="#启动hadoop" class="headerlink" title="启动hadoop"></a>启动hadoop</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">start-dfs.sh</div><div class="line">start-yarn.sh</div></pre></td></tr></table></figure>
<h3 id="格式化文件系统"><a href="#格式化文件系统" class="headerlink" title="格式化文件系统"></a>格式化文件系统</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hdfs namenode -format</div></pre></td></tr></table></figure>
<h3 id="建立用户空间（相当于连接了hadoop）"><a href="#建立用户空间（相当于连接了hadoop）" class="headerlink" title="建立用户空间（相当于连接了hadoop）"></a>建立用户空间（相当于连接了hadoop）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hdfs dfs -mkdir /user</div><div class="line">hdfs dfs -mkdir /user/$(whoami) # 这里是用户</div></pre></td></tr></table></figure>
<p>建立好目录后可以使用hadoop命令进行查看了</p>
<p>hadoop fs -ls /user/$(whoami)</p>
<h3 id="查看hadoop启动的进程情况"><a href="#查看hadoop启动的进程情况" class="headerlink" title="查看hadoop启动的进程情况"></a>查看hadoop启动的进程情况</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">jps</div></pre></td></tr></table></figure>
<h3 id="网页查看"><a href="#网页查看" class="headerlink" title="网页查看"></a>网页查看</h3><p>启动后可以在本地浏览器访问以下地址：</p>
<p><a href="http://localhost:8088/cluster" target="_blank" rel="external">http://localhost:8088/cluster</a></p>
<p><a href="http://localhost:50070" target="_blank" rel="external">http://localhost:50070</a></p>
<p><a href="http://localhost:8042/node" target="_blank" rel="external">http://localhost:8042/node</a></p>
<h2 id="Hadoop-Hello-World例程"><a href="#Hadoop-Hello-World例程" class="headerlink" title="Hadoop Hello World例程"></a>Hadoop Hello World例程</h2><p>利用自带的java程序测试，官方给了一个计算单词个数的代码也可以测试</p>
<p>###建立测试文件上传到HDFS中</p>
<p>在本地建立文件，我创建的文件与内容如下</p>
<p>file01</p>
<p>Hello World Bye World dfss<br>dfsa</p>
<p>file02</p>
<p>hello test</p>
<p>dfs0</p>
<p>上传文件命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hdfs dfs -put /User/liuce/input input #修改自己文件目录</div></pre></td></tr></table></figure>
<p>可以在刚才创建的目录下看到刚才上传的文件：/user/$(whoami)/input  #input自动生成的</p>
<h3 id="运行测试程序"><a href="#运行测试程序" class="headerlink" title="运行测试程序"></a>运行测试程序</h3><p>自带demo程序目录在</p>
<p>/usr/local/Cellar/hadoop/2.8.0/libexec/share/hadoop/mapreduce</p>
<p>运行测试程序</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop jar ./hadoop-mapreduce-examples-2.8.0.jar grep input output 'dfs[a-z.]+'</div></pre></td></tr></table></figure>
<p>测试程序是计算以dfs单词的个数，结果记录在/user/$(whoami)/out/part-r-00000</p>
<p>删除刚才生成的文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hdfs dfs -rm -r /user/$(whoami)/input</div><div class="line">hdfs dfs -rm -r /user/$(whoami)/output</div></pre></td></tr></table></figure>
<h2 id="快速搭建方式"><a href="#快速搭建方式" class="headerlink" title="快速搭建方式"></a>快速搭建方式</h2><p>在推荐两种快速的方式</p>
<ol>
<li><p>安装docker，基于docker的hadoop</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">docker pull sequenceiq/hadoop-docker:2.7.1</div><div class="line">docker run -it sequenceiq/hadoop-docker:2.7.1 /etc/bootstrap.sh -bash</div></pre></td></tr></table></figure>
</li>
<li><p>虚拟机直接启动</p>
<p>访问网站 <a href="https://bitnami.com/" target="_blank" rel="external">https://bitnami.com/</a> 搜索hadoop下载镜像，直接用相应的虚拟机启动。</p>
<p>运行hadoop启动命令就可以了，这种方式也可以方便搭建集群环境。</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Hadoop-安装&quot;&gt;&lt;a href=&quot;#Hadoop-安装&quot; class=&quot;headerlink&quot; title=&quot;Hadoop 安装&quot;&gt;&lt;/a&gt;Hadoop 安装&lt;/h2&gt;&lt;p&gt;系统macos 10.12.4，linux系统大体与此相似&lt;/p&gt;
&lt;figure 
    
    </summary>
    
    
      <category term="bigdata" scheme="http://cealiu.me/tags/bigdata/"/>
    
  </entry>
  
  <entry>
    <title>存储知识记录</title>
    <link href="http://cealiu.me/2017/09/03/%E5%AD%98%E5%82%A8%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/"/>
    <id>http://cealiu.me/2017/09/03/存储知识记录/</id>
    <published>2017-09-03T14:19:22.000Z</published>
    <updated>2017-09-03T14:22:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>平时工作是做云计算相关的，最近在学习hadoop的知识看到hadoop的文件存储方式结合用过的ceph，elasticsearch做下存储相关知识的记录。</p>
<h2 id="块存储、文件存储、对象存储"><a href="#块存储、文件存储、对象存储" class="headerlink" title="块存储、文件存储、对象存储"></a>块存储、文件存储、对象存储</h2><h3 id="介绍："><a href="#介绍：" class="headerlink" title="介绍："></a>介绍：</h3><p>块存储：是以扇区为基础的，一个或连续的扇区组成一个块，概念来自于物理存储。</p>
<p>文件储存：是多个物理块组成逻辑块后形成文件存储，根据不同的概念及驱动形成入nfs，ext4等文件系统。</p>
<p>对象存储：结合上面两个优点，增加了元数据(metadata)服务器。</p>
<p>这里有个知乎上不错的回答：<a href="http://www.zhihu.com/question/21536660" target="_blank" rel="external">http://www.zhihu.com/question/21536660</a></p>
<h3 id="优缺点："><a href="#优缺点：" class="headerlink" title="优缺点："></a>优缺点：</h3><p>推荐生产环境ceph使用块存储、对象存储</p>
<p>文件级备份：</p>
<p>文件级备份是指在指定某些文件进行备份时，首先会查找每个文件逻辑块，其次物理块，由于逻辑块是分散在物理块上，而物理块也是分散在不同扇区上。需要一层一 层往下查找，最后才完成整个文件复制。文件级备份时比较费时间，效率不高，实时性不强，备份时间长，且增量备份时，单文件某一小部份修改，不会只备份修改 部份，而整个文件都备份。</p>
<p>块级备份：</p>
<p>块级备份是指物理块复制，效率高，实时性强，备份时间短，且增量备份时，只备份修改过的物理块。</p>
<h2 id="ceph、hadoop、elasticsearch"><a href="#ceph、hadoop、elasticsearch" class="headerlink" title="ceph、hadoop、elasticsearch"></a>ceph、hadoop、elasticsearch</h2><p>hadoop：分布式存储主要适用于一次写入多次读取的场合（后续可能会增加其他数据处理方式），有数据块的概念(64M为一块，可配置)，将大文件分割为多个块进行存储；namenode内存中存放datanode数据索引，存储大小瓶颈来自namenode内存大小。</p>
<p>ceph：支持块存储、文件存储、对象存储；与hadoop相似的是块存储，不过更接近于物理块的概念；ceph的块驱动基于RBD（介绍<a href="http://www.sebastien-han.fr/blog/2016/03/28/ceph-jewel-preview-ceph-rbd-mirroring" target="_blank" rel="external">http://www.sebastien-han.fr/blog/2016/03/28/ceph-jewel-preview-ceph-rbd-mirroring</a>）</p>
<p>hadoop的存储也可以换成ceph的块存储不过性能可能会下降。</p>
<p>elasticsearch：更接近于nosql的数据库，不过分布式存储也是切片保存数据（介绍<a href="https://kibana.logstash.es/content/elasticsearch/principle/" target="_blank" rel="external">https://kibana.logstash.es/content/elasticsearch/principle/</a>）；查询的时候还有hadoop-elasticsearch插件感觉上是将logstash替换为了hadoop（理解的不知道对不对）。</p>
<h3 id="最后："><a href="#最后：" class="headerlink" title="最后："></a>最后：</h3><p>以上是工作中接触过的一些分布式存储的系统，要是想更深层次的理解一些知识还是要看一些理论行的东西如CAP，数据一致性存储等。</p>
<p>以上有什么说的不对的请指正，大家共同学习。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;平时工作是做云计算相关的，最近在学习hadoop的知识看到hadoop的文件存储方式结合用过的ceph，elasticsearch做下存储相关知识的记录。&lt;/p&gt;
&lt;h2 id=&quot;块存储、文件存储、对象存储&quot;&gt;&lt;a href=&quot;#块存储、文件存储、对象存储&quot; class=&quot;
    
    </summary>
    
    
      <category term="cloud" scheme="http://cealiu.me/tags/cloud/"/>
    
  </entry>
  
  <entry>
    <title>Logistic回归和Sigmoid函数的理解</title>
    <link href="http://cealiu.me/2017/09/01/Logistic%E5%9B%9E%E5%BD%92%E5%92%8CSigmoid%E5%87%BD%E6%95%B0%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>http://cealiu.me/2017/09/01/Logistic回归和Sigmoid函数的理解/</id>
    <published>2017-09-01T06:02:21.000Z</published>
    <updated>2017-09-03T14:13:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>看《机器学习实战》一书的Logistic回归记录下自己的理解，书上的例子用的是二维向量；该方法预测用的是Sigmoid函数（信号与系统的阶跃函数）。<br>$$<br>\sigma(z) = \cfrac{1}{1  +e^{-z}}<br>$$</p>
<p>Sigmoid函数的输入记为z，下面公式用于确定z：<br>$$<br>z = w_{0}x_{0} + w_{1}x_{1} + w_{2}x_{2} + …+ w_{n}x_{n}<br>$$</p>
<p>书上用的梯度上升算法确定系数w的数值。<br>$$<br>w = w + \alpha\delta f(w)<br>$$<br>该算法一直迭代找出w的系数，代码里是w（n，1），$\alpha = 0.001$ 循环500次得出系数矩阵。</p>
<p>该函数二维上就是在一些散点中画一条线来划分出不同的特征点，我们要找W就是这条线的系数。</p>
<p>书上给的给出的代码例程不能很好的运行，我做了简单的修改最后结果如下图：</p>
<p>修改过的源码上传到我的GitHub了<a href="https://github.com/cealiu/machinelearn/tree/master/Ch05" target="_blank" rel="external">地址</a></p>
<p><img src="/images/Logistic_1.png" alt=""></p>
<p>因为这个解法要迭代全部数据计算量比较大，后面的优化都是为了减少计算量做的比较好理解了就不在这里说明了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;看《机器学习实战》一书的Logistic回归记录下自己的理解，书上的例子用的是二维向量；该方法预测用的是Sigmoid函数（信号与系统的阶跃函数）。&lt;br&gt;$$&lt;br&gt;\sigma(z) = \cfrac{1}{1  +e^{-z}}&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;Sigmoi
    
    </summary>
    
    
      <category term="机器学习" scheme="http://cealiu.me/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>opentack节点扩容后热迁移异常</title>
    <link href="http://cealiu.me/2017/09/01/opentack%E8%8A%82%E7%82%B9%E6%89%A9%E5%AE%B9%E5%90%8E%E7%83%AD%E8%BF%81%E7%A7%BB%E5%BC%82%E5%B8%B8/"/>
    <id>http://cealiu.me/2017/09/01/opentack节点扩容后热迁移异常/</id>
    <published>2017-09-01T02:27:37.000Z</published>
    <updated>2017-09-01T02:47:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>扩容完成后旧计算节点上的虚机无法正常热迁移到新计算节点上</p>
<p>故障原因为：新旧节点的qemu版本存在差异。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rpm -qa | grep kvm</div></pre></td></tr></table></figure>
<p>qemu-kvm-2.5.1-1.1.el7.x86_64</p>
<p>libvirt-daemon-kvm-1.3.1-1.el7.centos.es.x86_64</p>
<p>qemu-kvm-common-2.5.1-1.1.el7.x86_64</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rpm -qa | grep kvm</div></pre></td></tr></table></figure>
<p>qemu-kvm-2.5.1-1.el7.es.x86_64</p>
<p>libvirt-daemon-kvm-1.3.1-1.el7.centos.es.x86_64</p>
<p>qemu-kvm-common-2.5.1-1.el7.es.x86_64</p>
<p>openstack虚机热迁移节点要求（摘录三条）:</p>
<ol>
<li><p>源和目标节点的 CPU 类型要一致。</p>
</li>
<li><p>源和目标节点的 Libvirt 版本要一致。</p>
</li>
<li><p>源和目标节点能相互识别对方的主机名称，比如可以在 /etc/hosts 中加入对方的条目。</p>
<p>迁移失败原因是第2条不满足。</p>
<p>升级qumu版本后解决，blog留作记录。</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;扩容完成后旧计算节点上的虚机无法正常热迁移到新计算节点上&lt;/p&gt;
&lt;p&gt;故障原因为：新旧节点的qemu版本存在差异。&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div cl
    
    </summary>
    
    
      <category term="cloud" scheme="http://cealiu.me/tags/cloud/"/>
    
  </entry>
  
  <entry>
    <title>python 发送邮件出现Connection reset by peer解决</title>
    <link href="http://cealiu.me/2017/08/31/python-%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%E5%87%BA%E7%8E%B0Connection-reset-by-peer%E8%A7%A3%E5%86%B3/"/>
    <id>http://cealiu.me/2017/08/31/python-发送邮件出现Connection-reset-by-peer解决/</id>
    <published>2017-08-31T13:17:37.000Z</published>
    <updated>2017-08-31T13:44:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>​       利用python自带的stmp模块发送邮件成功，但是在添加附件后发送失败。运行代码后提示出现connection reset by peer错误，查看后台服务器发现邮件服务器直接关闭了链接。</p>
<p>​       网上查找发现对于此问题的解释多是因为socket，web连接问题。后经排查后发现stmp协议使用的端口号是25：这里贴一段关于SMTP协议的介绍STMP协议介绍</p>
<p>​      下面连接邮件服务器一定要写上邮件端口号25<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">smtp.connect(smtpserver,<span class="string">"25"</span>)</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​       利用python自带的stmp模块发送邮件成功，但是在添加附件后发送失败。运行代码后提示出现connection reset by peer错误，查看后台服务器发现邮件服务器直接关闭了链接。&lt;/p&gt;
&lt;p&gt;​       网上查找发现对于此问题的解释多是因为
    
    </summary>
    
    
      <category term="python" scheme="http://cealiu.me/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://cealiu.me/2017/08/30/hello-world/"/>
    <id>http://cealiu.me/2017/08/30/hello-world/</id>
    <published>2017-08-30T12:19:34.000Z</published>
    <updated>2017-09-01T16:12:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>用hexo和github搭建了一个静态博客，程序员惯用helloworld例程那就拿之前在知乎写简单写的一个回答放在这里吧，当作熟悉hexo博客和markdown语法了；后续会将开源中国上的和笔记软件记录的内容放上来。</p>
<h2 id="韩国灾难电影推荐"><a href="#韩国灾难电影推荐" class="headerlink" title="韩国灾难电影推荐"></a>韩国灾难电影推荐</h2><p>最近在腾讯视频上看了部韩国电影《铁线虫入侵》仔细回想发现之前看过不少类似韩国灾难电影这里一并推荐出来吧。</p>
<h3 id="《铁线虫入侵》"><a href="#《铁线虫入侵》" class="headerlink" title="《铁线虫入侵》"></a>《铁线虫入侵》</h3><p>关于人体寄生虫的故事</p>
<p><img src="/images/tiexianchongruqin.jpg" alt=""></p>
<h3 id="《流感》"><a href="#《流感》" class="headerlink" title="《流感》"></a>《流感》</h3><p>讲传染病的故事，让人想起之前的非典不过电影比较夸张</p>
<p><img src="/images/liugan.jpg" alt=""></p>
<h3 id="《釜山行》"><a href="#《釜山行》" class="headerlink" title="《釜山行》"></a>《釜山行》</h3><p>号称亚洲首部丧尸片</p>
<p><img src="/images/fushanxing.jpg" alt=""></p>
<h3 id="《隧道》"><a href="#《隧道》" class="headerlink" title="《隧道》"></a>《隧道》</h3><p>类似井下求生的故事</p>
<p><img src="/images/suidao.jpg" alt=""></p>
<h3 id="《汉江怪物》"><a href="#《汉江怪物》" class="headerlink" title="《汉江怪物》"></a>《汉江怪物》</h3><p>大家一起打怪兽</p>
<p><img src="/images/hanjiangguaiwu.jpg" alt=""></p>
<h3 id="《潘多拉》"><a href="#《潘多拉》" class="headerlink" title="《潘多拉》"></a>《潘多拉》</h3><p>讲述核电站核泄漏的故事</p>
<p><img src="/images/panduola.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;用hexo和github搭建了一个静态博客，程序员惯用helloworld例程那就拿之前在知乎写简单写的一个回答放在这里吧，当作熟悉hexo博客和markdown语法了；后续会将开源中国上的和笔记软件记录的内容放上来。&lt;/p&gt;
&lt;h2 id=&quot;韩国灾难电影推荐&quot;&gt;&lt;a hr
    
    </summary>
    
    
      <category term="影视" scheme="http://cealiu.me/tags/%E5%BD%B1%E8%A7%86/"/>
    
  </entry>
  
</feed>
